import pretty_midi
import torch

def midi_to_notes(filepath):
    midi_data = pretty_midi.PrettyMIDI(filepath)
    notes = []
    for instrument in midi_data.instruments:
        if not instrument.is_drum:
            for note in instrument.notes:
                notes.append(note.pitch)  # MIDI pitch (0â€“127)
    return sorted(notes)

# Let's say we use sequences of 50 notes to predict the next note
seq_length = 50
step = 1


all_notes = midi_to_notes("C:\Users\benja\Downloads\Queen - Bohemian Rhapsody.mid")


# Encode note pitches to integer tokens
unique_notes = sorted(set(all_notes))
note_to_int = {note: i for i, note in enumerate(unique_notes)}
int_to_note = {i: note for note, i in note_to_int.items()}

# Convert notes to integers
encoded = [note_to_int[n] for n in all_notes]

# Create input/output pairs
X, y = [], []
for i in range(0, len(encoded) - seq_length, step):
    X.append(encoded[i:i+seq_length])
    y.append(encoded[i+seq_length])

X = torch.tensor(X)
y = torch.tensor(y)


import torch.nn as nn

class MusicRNN(nn.Module):
    def __init__(self, vocab_size, embed_size=100, hidden_size=256):
        super(MusicRNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x, hidden=None):
        x = self.embedding(x)
        out, hidden = self.lstm(x, hidden)
        out = self.fc(out)
        return out, hidden